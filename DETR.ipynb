{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8541e606-3955-4139-b04c-847f9eb73e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "import torchvision.transforms as T\n",
    "from torchvision import datasets, ops\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from einops import rearrange\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Category ID labels from Coco dataset\n",
    "CLASSES = [\"person\", \"bicycle\", \"car\", \"motorcycle\", \"airplane\", \"bus\", \"train\", \"truck\", \"boat\", \"traffic light\", \"fire hydrant\", \n",
    "           \"stop sign\", \"parking meter\", \"bench\", \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\", \"giraffe\", \n",
    "           \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\", \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \n",
    "           \"baseball glove\", \"skateboard\", \"surfboard\",\"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\", \"knife\", \"spoon\", \"bowl\", \n",
    "           \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\", \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"couch\", \n",
    "           \"potted plant\", \"bed\", \"dining table\", \"toilet\", \"tv\", \"laptop\", \"mouse\", \"remote\", \"keyboard\", \"cell phone\", \"microwave\", \n",
    "           \"oven\", \"toaster\", \"sink\", \"refrigerator\", \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\", \"empty\"]\n",
    "\n",
    "# Images revert normalisation before demo\n",
    "revertNormalisation = T.Normalize(mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225], std=[1/0.229, 1/0.224, 1/0.225])\n",
    "\n",
    "# Apple macbook metal performance shader\n",
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc25d4df-6d47-4c1e-aefe-495345e537e4",
   "metadata": {},
   "source": [
    "## Data preprocessing (Coco format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05105c3-1bad-44c0-aeee-fb03a882c73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessDataTargets(ann, w, h):\n",
    "    \"\"\"\n",
    "    Preprocessing image object annotations\n",
    "    \n",
    "    Args:\n",
    "        ann: path to json annotation file (train/validation)\n",
    "        w: width of the image\n",
    "        h: height of the image\n",
    "    Returns:\n",
    "        classes: valid category id labels for each image\n",
    "        bboxes: valid bounding boxes for each image\n",
    "    \"\"\"\n",
    "    # Extract annotations for each object in images\n",
    "    ann = [o for o in ann]\n",
    "    # Extract valid bounding boxes\n",
    "    bboxes = [o['bbox'] for o in ann]\n",
    "    bboxes = torch.as_tensor(bboxes, dtype=torch.float32).reshape(-1, 4)\n",
    "    # Bboxes format convert from xywh to x1y1x2y2\n",
    "    bboxes[:, 2:] += bboxes[:, :2]\n",
    "    bboxes[:, 0::2].clamp_(min=0, max=w)\n",
    "    bboxes[:, 1::2].clamp_(min=0, max=h)\n",
    "    # Mask for retaining valid bounding boxes and category labels\n",
    "    valid = (bboxes[:, 3]>bboxes[:, 1])&(bboxes[:, 2]>bboxes[:, 0])\n",
    "    bboxes = bboxes[valid]\n",
    "    \n",
    "    # Extract valid category label ids\n",
    "    classes = [o['category_id'] for o in ann]\n",
    "    classes = torch.tensor(classes, dtype=torch.int64)\n",
    "    classes = classes[valid]\n",
    "\n",
    "    # Scaling bboxes within [0, 1]\n",
    "    bboxes[:, 0::2]/=w\n",
    "    bboxes[:, 1::2]/=h\n",
    "    bboxes.clamp_(min=0, max=1)\n",
    "    # Bboxes format convert from x1y1x2y2 to cxcywh\n",
    "    bboxes = ops.box_convert(bboxes, in_fmt='xyxy', out_fmt='cxcywh')\n",
    "    return classes, bboxes\n",
    "\n",
    "class customCocoDetection(datasets.CocoDetection):\n",
    "    \"\"\"\n",
    "    Prepare dataset for dataloader\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Size of images to standardise (480*480)\n",
    "        self.size = 480\n",
    "        # Image resize and normalisaion\n",
    "        self.T = T.Compose([T.ToTensor(), T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), \n",
    "                            T.Resize((self.size, self.size), antialias=True)])\n",
    "        self.T_target = preprocessDataTargets\n",
    "    def __getitem__(self, idx):\n",
    "        image, target = super().__getitem__(idx)\n",
    "        width, height = image.size\n",
    "        _input = self.T(image)\n",
    "        classes, bboxes = self.T_target(target, width, height)\n",
    "        return _input, (classes, bboxes)\n",
    "        \n",
    "def collateFunction(inputs):\n",
    "    \"\"\"\n",
    "    Collate function for dataloader to process samples for batches\n",
    "    Args:\n",
    "        inputs: samples consist of images, category label ids and bounding boxes\n",
    "    Returns:\n",
    "        _input: stack of images\n",
    "        (classes, bboxes): tuples of category label ids and bounding boxes\n",
    "    \"\"\"\n",
    "    _input = torch.stack([i[0] for i in inputs])\n",
    "    classes = tuple([i[1][0] for i in inputs])\n",
    "    bboxes = tuple([i[1][1] for i in inputs])\n",
    "    return _input, (classes, bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f681da49-ded8-484a-8671-30e7848b8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = customCocoDetection(\"/Users/ivanng/dataset/coco-2017/train/data\", \"/Users/ivanng/dataset/coco-2017/raw/instances_train2017.json\")\n",
    "trainDataLoader = DataLoader(trainDataset, batch_size=16, shuffle=True, collate_fn=collateFunction)\n",
    "valDataset = customCocoDetection(\"/Users/ivanng/dataset/coco-2017/validation/data\", \"/Users/ivanng/dataset/coco-2017/raw/instances_val2017.json\")\n",
    "valDataLoader = DataLoader(valDataset, batch_size=2, shuffle=False, collate_fn=collateFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40266b01-e7be-4e20-b9f9-7b5a587538de",
   "metadata": {},
   "source": [
    "## DETR model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6d9605-0542-45dd-9a2d-465105a4e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHook(outputs, name):\n",
    "    def hook(self, input, output):\n",
    "        outputs[name] = output\n",
    "    return hook\n",
    "    \n",
    "class DETR(nn.Module):\n",
    "    def __init__(self, dimensions=256, numClasses=81, numTokens=225, numLayers=6, numHeads=8, numQueries=100):\n",
    "        super().__init__()\n",
    "        # ResNet50 Backbone for image feature extraction\n",
    "        self.backbone = create_feature_extractor(resnet50(weights=ResNet50_Weights.IMAGENET1K_V2), return_nodes={'layer4':'layer4'})\n",
    "        # 1*1 convolutional layer\n",
    "        self.conv11 = nn.Conv2d(2048, dimensions, kernel_size=1, stride=1)\n",
    "        # Learnable positional encoding during training\n",
    "        self.positionEncoding = nn.Parameter(torch.rand((1, numTokens, dimensions)), requires_grad=True)\n",
    "        # Transformer encoder\n",
    "        self.encoderLayer = nn.TransformerEncoderLayer(d_model=dimensions, nhead=numHeads, dim_feedforward=4*dimensions, batch_first=True, dropout=0.1)\n",
    "        self.transformerEncoder = nn.TransformerEncoder(self.encoderLayer, num_layers=numLayers)\n",
    "        # Learnable queries during training\n",
    "        self.queries = nn.Parameter(torch.rand((1, numQueries, dimensions)), requires_grad=True)\n",
    "        # Transformer decoder\n",
    "        self.decoderLayer = nn.TransformerDecoderLayer(d_model=dimensions, nhead=numHeads, dim_feedforward=4*dimensions, batch_first=True, dropout=0.1)\n",
    "        self.transformerDecoder = nn.TransformerDecoder(self.decoderLayer, num_layers=numLayers)\n",
    "        # Feed-forward Network heads\n",
    "        self.linearClass = nn.Linear(dimensions, numClasses)\n",
    "        self.linearBbox = nn.Linear(dimensions, 4)\n",
    "        # Hook to get intermediate outcome from each layer\n",
    "        self.decoder_outputs = {}\n",
    "        for idx, layer in enumerate(self.transformerDecoder.layers):\n",
    "            name = f'layer{idx}'\n",
    "            layer.register_forward_hook(getHook(self.decoder_outputs, name))\n",
    "        self.projection = nn.Linear(dimensions, dimensions)\n",
    "    def forward(self, x):\n",
    "        # (1, 3, 480, 480) input tensor to (1, 2048, 15, 15) feature map\n",
    "        embeddings = self.backbone(x)['layer4']\n",
    "        # Reduce channel dimensions from 2048 to 256\n",
    "        embeddings = self.conv11(embeddings)\n",
    "        # Rearrange embeddings to 1D sequence, tensor dimension transform from (1 256 15 15) to (1 225 256)\n",
    "        embeddings = rearrange(embeddings, 'b c h w -> b (h w) c')\n",
    "        # Encoder ouput dimension (1 225 256)\n",
    "        outputEncoder = self.transformerEncoder(embeddings+self.positionEncoding)\n",
    "        # Queries tensor and decoder output dimension (1 100 256)\n",
    "        outputDecoder = self.transformerDecoder(self.queries.repeat(len(outputEncoder), 1, 1), outputEncoder)\n",
    "        # Integrates outcomes from all intermediate decoder layers\n",
    "        outputs = {}\n",
    "        for i, o in self.decoder_outputs.items():\n",
    "            outputs[i] = {'category': self.linearClass(o), 'bbox': self.linearBbox(o)}\n",
    "        encoderEmbeddings = self.projection(outputEncoder)\n",
    "        decoderEmbeddings = self.projection(outputDecoder)  \n",
    "        embeddingsDict = {'encoderEmbeddings': encoderEmbeddings, 'decoderEmbeddings': decoderEmbeddings}\n",
    "        return outputs, embeddingsDict\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29a6335-ad8e-41d0-89e2-a28b5779dc74",
   "metadata": {},
   "source": [
    "## Loss computation for DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442102e6-32ec-4624-bb57-bfa0589187e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lossCompute(outputBbox, gtBbox, outputClass, gtClass, numQueries=100):\n",
    "    if len(gtBbox)>0:\n",
    "        gtBbox = gtBbox.to(device)\n",
    "        gtClass = gtClass.to(device)\n",
    "        outputProbs = outputClass.softmax(dim=-1)\n",
    "        # Compute costs for categories, bounding boxes and giou\n",
    "        classCost = -outputProbs[..., gtClass]\n",
    "        bboxCost = torch.cdist(outputBbox, gtBbox, p=1)\n",
    "        giouCost = -ops.generalized_box_iou(ops.box_convert(outputBbox, in_fmt='cxcywh', out_fmt='xyxy'),\n",
    "                                           ops.box_convert(gtBbox, in_fmt='cxcywh', out_fmt='xyxy'))\n",
    "        totalCost = 2*classCost+5*bboxCost+2*giouCost\n",
    "        totalCost = totalCost.cpu().detach().numpy()\n",
    "        # Optimal pairs compute minimal cost, returns pairs of indices\n",
    "        outputIndices, gtIndices = linear_sum_assignment(totalCost)\n",
    "        # Indices into tensors\n",
    "        outputIndices = torch.IntTensor(outputIndices)\n",
    "        gtIndices = torch.IntTensor(gtIndices)\n",
    "        # Sort output indices from model that align with ground truth category\n",
    "        outputIndices = outputIndices[gtIndices.argsort()]\n",
    "        # Compute losses for categories, bounding boxes and giou \n",
    "        numBboxes = len(gtBbox)\n",
    "        bboxLoss = F.l1_loss(outputBbox[outputIndices], gtBbox, reduction='sum')/numBboxes\n",
    "        giou = ops.generalized_box_iou(ops.box_convert(outputBbox[outputIndices], in_fmt='cxcywh', out_fmt='xyxy'), \n",
    "                                      ops.box_convert(gtBbox, in_fmt='cxcywh', out_fmt='xyxy'))\n",
    "        # Diagonal contains Bipartite pairs, 1-giou = giouloss\n",
    "        giouLoss = 1-torch.diag(giou).mean()\n",
    "        # Assign no-object class since actual number predictions less than number of queries\n",
    "        queriesClassesLabel = torch.full(outputProbs.shape[:1], 81).to(device)\n",
    "        queriesClassesLabel[outputIndices] = gtClass\n",
    "        classLoss = F.cross_entropy(outputClass, queriesClassesLabel)\n",
    "    else:\n",
    "        queriesClassesLabel = torch.full((numQueries,), 81).to(device)\n",
    "        classLoss = F.cross_entropy(outputClass, queriesClassesLabel)\n",
    "        bboxLoss = giouLoss = torch.tensor(0)\n",
    "    return classLoss, bboxLoss, giouLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0349bfd-fbe8-4f60-b31c-f65ed15574bd",
   "metadata": {},
   "source": [
    "## Training DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c38afd-ede7-4e1d-8f4a-f7472b1e7323",
   "metadata": {},
   "outputs": [],
   "source": [
    "detr = DETR(dimensions=256, numClasses=81, numTokens=225, numLayers=6, numHeads=8, numQueries=100).to(device)\n",
    "backboneParameters = [p for n, p in detr.named_parameters() if 'backbone.' in n]\n",
    "\n",
    "for p in detr.backbone.parameters():\n",
    "   p.requires_grad = False\n",
    "    \n",
    "transformerParameters = [p for n, p in detr.named_parameters() if 'backbone.' not in n]\n",
    "optimizer = AdamW([{'params': transformerParameters, 'lr': 1e-4}, {'params': backboneParameters, 'lr': 1e-5}], weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf1a325-223e-4f13-bf7a-2acadd0a3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(True)\n",
    "detr.train()\n",
    "numEpochs = 141\n",
    "batchSize=16\n",
    "losses = []\n",
    "history = []\n",
    "\n",
    "for i in tqdm(range(len(history)+1, numEpochs)):\n",
    "    for _input, (gtClasses, gtBboxes) in trainDataLoader:\n",
    "        _input = _input.to(device)\n",
    "        outputs, _ = detr(_input)\n",
    "        loss = torch.Tensor([0]).to(device)\n",
    "        for name, output in outputs.items():\n",
    "            output['bbox'] = output['bbox'].sigmoid()\n",
    "            for predictBbox, gtBbox, predictClass, gtClass in zip(output['bbox'], gtBboxes, output['category'], gtClasses):\n",
    "                classLoss, bboxLoss, giouLoss = lossCompute(predictBbox, gtBbox, predictClass, gtClass)\n",
    "                sampleTotalLoss = 1*classLoss+5*bboxLoss+2*giouLoss\n",
    "                loss += sampleTotalLoss/batchSize\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(detr.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    if i%1==0:\n",
    "        avgLoss = np.mean(losses)\n",
    "        print(f'epoch: {i}, loss: {avgLoss:.4f}')\n",
    "        print(f'classLoss: {classLoss.item():.4f}, bboxLoss: {bboxLoss.item():.4f}, giouLoss: {giouLoss.item():.4f}')\n",
    "        history.append(avgLoss)\n",
    "        losses = []\n",
    "    if i%1==0:\n",
    "        torch.save({'state': detr.state_dict(), 'optimizerState': optimizer.state_dict()}, f'/Users/ivanng/model/RT_DETR_epoch{i}.pt')\n",
    "        np.save(f'/Users/ivanng/model/history_epoch{i}.npy', history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
