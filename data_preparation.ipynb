{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf743bbf-c3d7-4909-a026-162bffa498d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#dataset = foz.load_zoo_dataset(\"coco-2017\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36566042-f3c9-4e0b-9829-f45a32b4c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract video frames\n",
    "def extract_videoframes(videoPath):\n",
    "    capture = cv2.VideoCapture(videoPath)\n",
    "    frames = []\n",
    "    while capture.isOpened():\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            print(\"Error: cannot receive frame\")\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    capture.release()\n",
    "    return frames\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241563b-be77-4149-930d-1fed492a060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_images_dir = \"/Users/ivanng/dataset/coco-2017/train/data\"\n",
    "coco_ann_file_caption = \"/Users/ivanng/dataset/coco-2017/raw/captions_train2017.json\"\n",
    "coco_ann_file_detection = \"/Users/ivanng/dataset/coco-2017/raw/instances_train2017.json\"\n",
    "\n",
    "#Load Coco annotations\n",
    "def loadCocoAnnotations(coco_ann_file_detection):\n",
    "    #Load COCO image annotation\n",
    "    coco = COCO(coco_ann_file_detection)\n",
    "    #Obtain category information\n",
    "    categories = coco.loadCats(coco.getCatIds())\n",
    "    categoryMap = {category['id']:category['name'] for category in categories}\n",
    "    return coco, categoryMap\n",
    "\n",
    "#Preprocess images\n",
    "def preprocessImage(image, targetSize=(128,128)):\n",
    "    height, width = image.shape[:2]\n",
    "    scale = min(targetSize[0]/height, targetSize[1]/width)\n",
    "    scaledHeight, scaledWidth = int(height*scale), int(width*scale)\n",
    "    #Resize image\n",
    "    resizedImage = cv2.resize(image, (scaledWidth, scaledHeight))\n",
    "    #Padding image to targetSize\n",
    "    imagePadded = np.full((targetSize[0], targetSize[1], 3), 128, dtype=np.uint8)\n",
    "    x_offset = (targetSize[1]-scaledWidth)//2\n",
    "    y_offset = (targetSize[0]-scaledHeight)//2\n",
    "    imagePadded[y_offset:y_offset+scaledHeight, x_offset:x_offset+scaledWidth] = resizedImage\n",
    "\n",
    "    #Normalise to [0, 1]\n",
    "    imagePadded = imagePadded/255.0\n",
    "    return imagePadded, scale, x_offset, y_offset\n",
    "\n",
    "#Preprocess bounding boxes\n",
    "def preprocessBboxes(bboxes, scale, x_offset, y_offset):\n",
    "    newBboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x, y, width, height = bbox\n",
    "        xStart = x*scale+x_offset\n",
    "        yStart = y*scale+y_offset\n",
    "        xEnd = xStart+width*scale\n",
    "        yEnd = yStart+height*scale\n",
    "        newBboxes.append([xStart, yStart, xEnd, yEnd])\n",
    "    return newBboxes\n",
    "\n",
    "#Get entire preprocess data\n",
    "def getImageAnnotations(coco, coco_images_dir, categoryMap, targetSize=(128, 128)):\n",
    "    data = []\n",
    "    for i, imageID in enumerate(coco.getImgIds()):\n",
    "        imageInfo = coco.loadImgs(imageID)[0]\n",
    "        #Load image\n",
    "        imagePath = os.path.join(coco_images_dir, imageInfo['file_name'])\n",
    "        image = cv2.imread(imagePath)\n",
    "        if image is None: continue\n",
    "        annotations = coco.loadAnns(coco.getAnnIds(imgIds=imageID))\n",
    "        #Obtain labels, bboxes\n",
    "        labels = [categoryMap[ann['category_id']] for ann in annotations if 'category_id' in ann]\n",
    "        bboxes = [ann['bbox'] for ann in annotations if 'bbox' in ann]\n",
    "        #Invoke image preprocess\n",
    "        preprocessedImage, scale, x_offset, y_offset = preprocessImage(image, targetSize)\n",
    "        #Invoke bboxes preprocess\n",
    "        preprocessedBboxes = preprocessBboxes(bboxes, scale, x_offset, y_offset)\n",
    "        \n",
    "        data.append((preprocessedImage, preprocessedBboxes, labels))\n",
    "\n",
    "        if i<5:\n",
    "            visualise(preprocessedImage, preprocessedBboxes, labels)\n",
    "    return data\n",
    "\n",
    "#Temporary visualisation of bboxes and labels on images\n",
    "def visualise(image, bboxes, labels):\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    for i, box in enumerate(bboxes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        label = labels[i]\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "        cv2.putText(image, labels[i], (x1, y1-2), cv2.FONT_HERSHEY_SIMPLEX, 0.2, (0, 255, 0), 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315beb1d-096d-4677-94b4-da7434bac768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detection model framework\n",
    "def detectionModel(numClasses, inputShape=(128,128,3)):\n",
    "    inputs = tf.keras.Input(shape=inputShape)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    x = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    outputBbox = tf.keras.layers.Dense(4, activation='sigmoid')(x)\n",
    "    outputClass = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[outputBbox, outputClass])\n",
    "    return model\n",
    "#Model training\n",
    "def modelTraining(model, data, num_classes, epochs=):\n",
    "    images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86602b-7129-458e-8402-ef525e59240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary execution\n",
    "coco, categoryMap = loadCocoAnnotations(coco_ann_file_detection)\n",
    "data = getImageAnnotations(coco, coco_images_dir, categoryMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6295937-7905-4850-be83-922a18ca898a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
