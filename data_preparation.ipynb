{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf743bbf-c3d7-4909-a026-162bffa498d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from pycocotools.coco import COCO\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "#ds = load_dataset(\"chengyenhsieh/TAO-Amodal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36566042-f3c9-4e0b-9829-f45a32b4c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract video frames\n",
    "def extract_videoframes(videoPath):\n",
    "    capture = cv2.VideoCapture(videoPath)\n",
    "    frames = []\n",
    "    while capture.isOpened():\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            print(\"Error: cannot receive frame\")\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    capture.release()\n",
    "    return frames\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241563b-be77-4149-930d-1fed492a060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_images_dir = \"/Users/ivanng/dataset/coco-2017/train/data\"\n",
    "coco_ann_file_caption = \"/Users/ivanng/dataset/coco-2017/raw/captions_train2017.json\"\n",
    "coco_ann_file_detection = \"/Users/ivanng/dataset/coco-2017/raw/instances_train2017.json\"\n",
    "\n",
    "#Preprocess images\n",
    "def preprocessImage(image, targetSize=(128,128), paddingType='zero'):\n",
    "    \"\"\"\n",
    "    Resize and pad an image to target size\n",
    "    Args:\n",
    "        image: input image ready for preprocessing\n",
    "        targetSize: resize the image to the ideal size\n",
    "        paddingType: type of padding would implement\n",
    "    Returns:\n",
    "        imagePadded: padded image\n",
    "        scale: scale factor for image width and height\n",
    "        x_offset, y_offset: padding offsets\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    scale = min(targetSize[0]/height, targetSize[1]/width)\n",
    "    scaledHeight, scaledWidth = int(height*scale), int(width*scale)\n",
    "    x_offset = (targetSize[1]-scaledWidth)//2\n",
    "    y_offset = (targetSize[0]-scaledHeight)//2\n",
    "    #Resize image\n",
    "    resizedImage = cv2.resize(image, (scaledWidth, scaledHeight))\n",
    "    #Create padding for resized image\n",
    "    if paddingType=='zero':\n",
    "        imagePadded = np.zeros((targetSize[0], targetSize[1], 3), dtype=np.uint8)\n",
    "    elif paddingType=='mirror':\n",
    "        imagePadded = cv2.copyMakeBorder(resizedImage, y_offset, targetSize[0]-scaledHeight-y_offset, x_offset, targetSize[1]-scaledWidth-x_offset, borderType=cv2.BORDER_REFLECT)\n",
    "        return imagePadded/255, scale, x_offset, y_offset\n",
    "    elif paddingType==\"replicate\":\n",
    "        imagePadded = cv2.copyMakeBorder(resizedImage, y_offset, targetSize[0]-scaledHeight-y_offset, x_offset, targetSize[1]-scaledWidth-x_offset, borderType=cv2.BORDER_REPLICATE)\n",
    "        return imagePadded/255, scale, x_offset, y_offset\n",
    "    imagePadded[y_offset:y_offset+scaledHeight, x_offset:x_offset+scaledWidth] = resizedImage\n",
    "    #Normalise to [0, 1]\n",
    "    imagePadded = imagePadded/255\n",
    "    return imagePadded, scale, x_offset, y_offset\n",
    "\n",
    "#Preprocess bounding boxes\n",
    "def preprocessBboxes(bboxes, scale, x_offset, y_offset):\n",
    "    \"\"\"\n",
    "    Adjust bboxes after resized image\n",
    "    Args:\n",
    "        bboxes: bounding boxes for preprocessing\n",
    "        scale: scale factor\n",
    "        x_offset, y_offset: padding offsets\n",
    "    Returns:\n",
    "        newBboxes: a list of bounding boxes\n",
    "    \"\"\"\n",
    "    newBboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x, y, width, height = bbox\n",
    "        xStart = x*scale+x_offset\n",
    "        yStart = y*scale+y_offset\n",
    "        xEnd = xStart+width*scale\n",
    "        yEnd = yStart+height*scale\n",
    "        newBboxes.append([xStart, yStart, xEnd, yEnd])\n",
    "    return newBboxes\n",
    "\n",
    "#Get entire preprocess data\n",
    "def getImageAnnotations(coco_ann, coco_images_dir, targetSize=(128, 128), paddingType='zero'):\n",
    "    \"\"\"\n",
    "    Preprocess MS COCO dataset\n",
    "    Args:\n",
    "        coco_ann: json annotation file directory\n",
    "        coco_images_dir: image directory\n",
    "        targetSize: size of standardised image\n",
    "        paddingType: type of padding used\n",
    "    Returns:\n",
    "        data: a list of preprocessed images and annotations\n",
    "    \"\"\"\n",
    "    coco = COCO(coco_ann)\n",
    "    data = []\n",
    "    for imageID in coco.getImgIds():\n",
    "        imageInfo = coco.loadImgs(imageID)[0]\n",
    "        #Load image\n",
    "        imagePath = os.path.join(coco_images_dir, imageInfo['file_name'])\n",
    "        image = cv2.imread(imagePath)\n",
    "        if image is None: continue\n",
    "        #Invoke image preprocess\n",
    "        preprocessedImage, scale, x_offset, y_offset = preprocessImage(image, targetSize, paddingType)\n",
    "        #Extract annotations for corresponding image\n",
    "        annotations = coco.loadAnns(coco.getAnnIds(imgIds=imageID))\n",
    "        #Obtain labels, bboxes\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            #Invoke bboxes preprocess\n",
    "            preprocessedBboxes = preprocessBboxes([ann['bbox']], scale, x_offset, y_offset)[0]\n",
    "            bboxes.append(preprocessedBboxes)\n",
    "            labels.append(ann['category_id'])\n",
    "        data.append((preprocessedImage, np.array(bboxes), np.array(labels)))\n",
    "    return data\n",
    "\n",
    "#TEMPORARY: visualisation of bboxes and labels on images\n",
    "def visualise(image, bboxes, labels):\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    for i, box in enumerate(bboxes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        label = labels[i]\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "        cv2.putText(image, labels[i], (x1, y1-2), cv2.FONT_HERSHEY_SIMPLEX, 0.2, (0, 255, 0), 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315beb1d-096d-4677-94b4-da7434bac768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real-time detection transformer framework\n",
    "def RT_DETR(numClasses, inputShape=(128,128,3)):\n",
    "    \"\"\"\n",
    "    RT-DETR framework\n",
    "    Args:\n",
    "        numClasses: number of object classes to classify\n",
    "        inputShape: input image dimensions\n",
    "    Returns:\n",
    "        model: real-time detection transformer model\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=inputShape)\n",
    "    #CNN backbone\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    #Flatten feature map for encoder-decorder\n",
    "    featureMap = tf.keras.layers.Reshape((-1, x.shape[-1]))(x)\n",
    "    #Encoder\n",
    "    encoder = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=64)(featureMap, featureMap)\n",
    "    encoder = tf.keras.layers.LayerNormalisation()(encoder)\n",
    "    #Query selection\n",
    "    query = tf.keras.layers.Dense(64)(tf.keras.layers.Flatten()(featureMap))\n",
    "    #Decoder\n",
    "    decoder = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=64)(query, encoder)\n",
    "    decoder = tf.keras.layers.LayerNormalisation()(decoder) \n",
    "    #Detection heads\n",
    "    outputBbox = tf.keras.layers.Dense(4, activation='sigmoid')(decoder)\n",
    "    outputClass = tf.keras.layers.Dense(numClasses, activation='softmax')(decoder)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[outputBbox, outputClass])\n",
    "    return model\n",
    "    \n",
    "#Re-ID network\n",
    "def Re_ID(inputShape=(64,64,3)):\n",
    "    \"\"\"\n",
    "    Re-Identification network\n",
    "    Args:\n",
    "        inputShape: dimensions of cropped input images\n",
    "    Returns:\n",
    "        model: Re-Identification network\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.layers.Input(shape=inputShape)\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    embeddings = tf.keras.layers.Dense(128, activation='relu')(x)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=embeddings)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb980df8-de4c-4940-a661-d0165164a836",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepSORT:\n",
    "    def __init__(self, ReID, iou_threshold, max_age):\n",
    "        \"\"\"\n",
    "        Initialise Deep SORT tracker, combines IoU matching and Re-Identification embeddings to track objs across frames\n",
    "        Args:\n",
    "            ReID: trained Re-Identification network\n",
    "            iou_threshold: IoU threshold for matching detections\n",
    "            max_age: maximum frames for retaining unmatched tracks\n",
    "        \"\"\"\n",
    "        self.model = ReID\n",
    "        self.iouThreshold = iou_threshold\n",
    "        self.maxAge = max_age\n",
    "        self.tracks = []\n",
    "        self.nextTrackID = 1\n",
    "    def iou(self, box1, box2):\n",
    "        \"\"\"\n",
    "        Compute intersection over union between two bboxes\n",
    "        Args:\n",
    "            box1, box2: two input bboxes\n",
    "        Returns:\n",
    "            iou: IoU value\n",
    "        \"\"\"\n",
    "        #Compute intersection coordinates\n",
    "        x1 = max(box1[0], box2[0])\n",
    "        y1 = max(box1[1], box2[1])\n",
    "        x2 = min(box1[2], box2[2])\n",
    "        y2 = min(box1[3], box2[3])\n",
    "        #Area of intersection\n",
    "        intersects = max(0, x2-x1)*max(0, y2-y1)\n",
    "        #Area of both two bboxes\n",
    "        box1Area = (box1[2]-box1[0])*(box1[3]-box1[1])\n",
    "        box2Area = (box2[2]-box2[0])*(box2[3]-box2[1])\n",
    "        #Compute IoU value\n",
    "        iou = intersects/float(box1Area+box2Area-intersects)\n",
    "        return iou\n",
    "    def trackMatch(self, detections):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            detections: a list of detection bboxes\n",
    "        Returns:\n",
    "            matchedTracks: a list of matched tracks\n",
    "            unmatchedTracks: a list of unmatched tracks\n",
    "            unmatchedDetections: a list of unmatched bboxes detections\n",
    "        \"\"\"\n",
    "        if len(self.tracks)==0: return [], list(range(len(detections))), []\n",
    "        #Cost matrix\n",
    "        iouMat = np.zeros((len(self.tracks), len(detections)))\n",
    "        for i, track in enumerate(self.tracks):\n",
    "            for j, detect in enumerate(detections):\n",
    "                iouMat[i, j] = self.iou(track['bbox'], detect)\n",
    "        #Hungarian maximum matching algorithm\n",
    "        rowIndices, columnIndices = linear_sum_assignment(-iouMat)\n",
    "        \n",
    "        matchedTracks, unmatchedTracks, unmatchedDetections = [], [], list(range(len(detections)))\n",
    "        #Matching detection\n",
    "        for rIdx, cIdx in zip(rowIndices, columnIndices):\n",
    "            if iouMat[rIdx,cIdx]<self.iouThreshold:\n",
    "                unmatchedTracks.append(rIdx)\n",
    "                unmatchedDetections.append(cIdx)\n",
    "            else:\n",
    "                matchedTracks.append((rIdx, cIdx))\n",
    "                unmatchedDetections.remove(cIdx)\n",
    "        #Add Unmatched tracks\n",
    "        unmatchedTracks += list(set(range(len(self.tracks)))-set(rowIndices))\n",
    "        return matchedTracks, unmatchedTracks, unmatchedDetections\n",
    "    def update(self, detections, embeddings):\n",
    "        \"\"\"\n",
    "        Update tracking with detections and embeddings\n",
    "        Args:\n",
    "            detections: a list of detection bboxes\n",
    "            embeddings: a list of Re-Identification embeddings for each detection\n",
    "        Returns:\n",
    "            a list of updated trackings\n",
    "        \"\"\"\n",
    "        #Match detections to existing trackings\n",
    "        matchedTracks, unmatchedTracks, unmatchedDetections = self.trackMatch(detections)\n",
    "        #Update matched trackings\n",
    "        for trackIdx, detectionIdx in matchedTracks:\n",
    "            self.tracks[trackIdx]['bbox'] = detections[detectionIdx]\n",
    "            self.tracks[trackIdx]['embedding'] = embeddings[detectionIdx] \n",
    "            self.tracks[trackIdx]['age'] = 0\n",
    "        #Age unseen trackings\n",
    "        for trackIdx in unmatchedTracks:\n",
    "            self.tracks[trackIdx]['age'] += 1\n",
    "        #Remove any unnecessary trackings \n",
    "        self.tracks = [track for track in self.tracks if track['age']<=self.maxAge]\n",
    "        #Add new trackings\n",
    "        for detectionIdx in unmatchedDetections:\n",
    "            self.tracks.append({'id':self.nextTrackID, 'bbox':detections[detectionIdx], 'embedding':embeddings[detectionIdx], 'age':0})\n",
    "            self.nextTrackID+=1\n",
    "        return self.tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ed0642-41f7-4cfc-82c7-8b594a082473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86602b-7129-458e-8402-ef525e59240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary execution\n",
    "coco, categoryMap = loadCocoAnnotations(coco_ann_file_detection)\n",
    "data = getImageAnnotations(coco, coco_images_dir, categoryMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6295937-7905-4850-be83-922a18ca898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
