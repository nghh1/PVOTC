{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf743bbf-c3d7-4909-a026-162bffa498d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "from pycocotools.coco import COCO\n",
    "\n",
    "#ds = load_dataset(\"chengyenhsieh/TAO-Amodal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36566042-f3c9-4e0b-9829-f45a32b4c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract video frames\n",
    "def extract_videoframes(videoPath):\n",
    "    capture = cv2.VideoCapture(videoPath)\n",
    "    frames = []\n",
    "    while capture.isOpened():\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            print(\"Error: cannot receive frame\")\n",
    "            break\n",
    "        frames.append(frame)\n",
    "    capture.release()\n",
    "    return frames\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241563b-be77-4149-930d-1fed492a060e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_images_dir = \"/Users/ivanng/dataset/coco-2017/train/data\"\n",
    "coco_ann_file_caption = \"/Users/ivanng/dataset/coco-2017/raw/captions_train2017.json\"\n",
    "coco_ann_file_detection = \"/Users/ivanng/dataset/coco-2017/raw/instances_train2017.json\"\n",
    "\n",
    "#Preprocess images\n",
    "def preprocessImage(image, targetSize=(128,128), paddingType='zero'):\n",
    "    \"\"\"\n",
    "    Resize and pad an image to target size\n",
    "    Args:\n",
    "        image: input image ready for preprocessing\n",
    "        targetSize: resize the image to the ideal size\n",
    "        paddingType: type of padding would implement\n",
    "    Returns:\n",
    "        imagePadded: padded image\n",
    "        scale: scale factor for image width and height\n",
    "        x_offset, y_offset: padding offsets\n",
    "    \"\"\"\n",
    "    height, width = image.shape[:2]\n",
    "    scale = min(targetSize[0]/height, targetSize[1]/width)\n",
    "    scaledHeight, scaledWidth = int(height*scale), int(width*scale)\n",
    "    x_offset = (targetSize[1]-scaledWidth)//2\n",
    "    y_offset = (targetSize[0]-scaledHeight)//2\n",
    "    #Resize image\n",
    "    resizedImage = cv2.resize(image, (scaledWidth, scaledHeight))\n",
    "    #Create padding for resized image\n",
    "    if paddingType=='zero':\n",
    "        imagePadded = np.zeros((targetSize[0], targetSize[1], 3), dtype=np.uint8)\n",
    "    elif paddingType=='mirror':\n",
    "        imagePadded = cv2.copyMakeBorder(resizedImage, y_offset, targetSize[0]-scaledHeight-y_offset, x_offset, targetSize[1]-scaledWidth-x_offset, borderType=cv2.BORDER_REFLECT)\n",
    "        return imagePadded/255, scale, x_offset, y_offset\n",
    "    elif paddingType==\"replicate\":\n",
    "        imagePadded = cv2.copyMakeBorder(resizedImage, y_offset, targetSize[0]-scaledHeight-y_offset, x_offset, targetSize[1]-scaledWidth-x_offset, borderType=cv2.BORDER_REPLICATE)\n",
    "        return imagePadded/255, scale, x_offset, y_offset\n",
    "    imagePadded[y_offset:y_offset+scaledHeight, x_offset:x_offset+scaledWidth] = resizedImage\n",
    "    #Normalise to [0, 1]\n",
    "    imagePadded = imagePadded/255\n",
    "    return imagePadded, scale, x_offset, y_offset\n",
    "\n",
    "#Preprocess bounding boxes\n",
    "def preprocessBboxes(bboxes, scale, x_offset, y_offset):\n",
    "    \"\"\"\n",
    "    Adjust bboxes after resized image\n",
    "    Args:\n",
    "        bboxes: bounding boxes for preprocessing\n",
    "        scale: scale factor\n",
    "        x_offset, y_offset: padding offsets\n",
    "    Returns:\n",
    "        newBboxes: a list of bounding boxes\n",
    "    \"\"\"\n",
    "    newBboxes = []\n",
    "    for bbox in bboxes:\n",
    "        x, y, width, height = bbox\n",
    "        xStart = x*scale+x_offset\n",
    "        yStart = y*scale+y_offset\n",
    "        xEnd = xStart+width*scale\n",
    "        yEnd = yStart+height*scale\n",
    "        newBboxes.append([xStart, yStart, xEnd, yEnd])\n",
    "    return newBboxes\n",
    "\n",
    "#Get entire preprocess data\n",
    "def getImageAnnotations(coco_ann, coco_images_dir, targetSize=(128, 128), paddingType='zero'):\n",
    "    \"\"\"\n",
    "    Preprocess MS COCO dataset\n",
    "    Args:\n",
    "        coco_ann: json annotation file directory\n",
    "        coco_images_dir: image directory\n",
    "        targetSize: size of standardised image\n",
    "        paddingType: type of padding used\n",
    "    Returns:\n",
    "        data: a list of preprocessed images and annotations\n",
    "    \"\"\"\n",
    "    coco = COCO(coco_ann)\n",
    "    data = []\n",
    "    for imageID in coco.getImgIds():\n",
    "        imageInfo = coco.loadImgs(imageID)[0]\n",
    "        #Load image\n",
    "        imagePath = os.path.join(coco_images_dir, imageInfo['file_name'])\n",
    "        image = cv2.imread(imagePath)\n",
    "        if image is None: continue\n",
    "        #Invoke image preprocess\n",
    "        preprocessedImage, scale, x_offset, y_offset = preprocessImage(image, targetSize, paddingType)\n",
    "        #Extract annotations for corresponding image\n",
    "        annotations = coco.loadAnns(coco.getAnnIds(imgIds=imageID))\n",
    "        #Obtain labels, bboxes\n",
    "        bboxes = []\n",
    "        labels = []\n",
    "        for ann in annotations:\n",
    "            #Invoke bboxes preprocess\n",
    "            preprocessedBboxes = preprocessBboxes([ann['bbox']], scale, x_offset, y_offset)[0]\n",
    "            bboxes.append(preprocessedBboxes)\n",
    "            labels.append(ann['category_id'])\n",
    "        data.append((preprocessedImage, np.array(bboxes), np.array(labels)))\n",
    "    return data\n",
    "\n",
    "#TEMPORARY: visualisation of bboxes and labels on images\n",
    "def visualise(image, bboxes, labels):\n",
    "    image = (image*255).astype(np.uint8)\n",
    "    for i, box in enumerate(bboxes):\n",
    "        x1, y1, x2, y2 = map(int, box)\n",
    "        label = labels[i]\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 1)\n",
    "        cv2.putText(image, labels[i], (x1, y1-2), cv2.FONT_HERSHEY_SIMPLEX, 0.2, (0, 255, 0), 1)\n",
    "    plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315beb1d-096d-4677-94b4-da7434bac768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Real-time detection transformer framework\n",
    "def RT_DETR(numClasses, inputShape=(128,128,3)):\n",
    "    \"\"\"\n",
    "    RT-DETR framework\n",
    "    \"\"\"\n",
    "    inputs = tf.keras.Input(shape=inputShape)\n",
    "    #CNN backbone\n",
    "    x = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    x = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = tf.keras.layers.MaxPooling2D((2,2))(x)\n",
    "    #Flatten feature map for encoder-decorder\n",
    "    featureMap = tf.keras.layers.Reshape((-1, x.shape[-1]))(x)\n",
    "    #Encoder\n",
    "    encoder = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=64)(featureMap, featureMap)\n",
    "    encoder = tf.keras.layers.LayerNormalisation()(encoder)\n",
    "    #Query selection\n",
    "    query = tf.keras.layers.Dense(64)(tf.keras.layers.Flatten()(featureMap))\n",
    "    #Decoder\n",
    "    decoder = tf.keras.layers.MultiHeadAttention(num_heads=4, key_dim=64)(query, encoder)\n",
    "    decoder = tf.keras.layers.LayerNormalisation()(decoder) \n",
    "    #Detection heads\n",
    "    outputBbox = tf.keras.layers.Dense(4, activation='sigmoid')(decoder)\n",
    "    outputClass = tf.keras.layers.Dense(numClasses, activation='softmax')(decoder)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[outputBbox, outputClass])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac86602b-7129-458e-8402-ef525e59240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temporary execution\n",
    "coco, categoryMap = loadCocoAnnotations(coco_ann_file_detection)\n",
    "data = getImageAnnotations(coco, coco_images_dir, categoryMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6295937-7905-4850-be83-922a18ca898a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
