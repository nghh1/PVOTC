{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6cd81-1bed-4294-b173-ef10374947c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import RTDetrForObjectDetection, RTDetrImageProcessor\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import clip\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ba189a-799a-4eb8-9626-dbdb838c0f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, device=torch.device(\"mps\")):\n",
    "        self.device = device\n",
    "        self.initialiseModels()\n",
    "        \n",
    "    def initialiseModels(self):\n",
    "        self.detectionModel = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\").to(self.device)\n",
    "        self.detectionModelImageProcessor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd_coco_o365\")\n",
    "        self.clipModel, self.clip_preprocess = clip.load(\"ViT-B/32\", device=self.device)\n",
    "        self.captionModel = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16).to(self.device)\n",
    "        self.captionProcessor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "        \n",
    "        self.boxAnnotator = sv.BoxAnnotator()\n",
    "        self.labelAnnotator = sv.LabelAnnotator()\n",
    "        self.captionCache = []\n",
    "    \n",
    "    def processVideo(self, videoPath, query, outputPath, captionInterval, similarity, captionsFile=\"captions.txt\"):\n",
    "        \"\"\"Main function for single pass execution\"\"\"\n",
    "        queryEmbedding = self.encodeQuery(query)\n",
    "        capture = cv2.VideoCapture(videoPath)\n",
    "        fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        totalFrames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        self.tracker = sv.ByteTrack(track_activation_threshold=0.4, lost_track_buffer=fps, minimum_matching_threshold=0.7, frame_rate=fps)\n",
    "        videoWriter = cv2.VideoWriter(outputPath, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "        captionFrames = []\n",
    "        currentBatch = []\n",
    "        frameCount = 0\n",
    "        with open(captionsFile, 'w') as cf:\n",
    "            with tqdm(total=totalFrames, desc=\"Processing Video\") as bar:\n",
    "                while capture.isOpened():\n",
    "                    ret, frame = capture.read()\n",
    "                    if not ret: break\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    currentTime = frameCount/fps\n",
    "                    # Caption generation\n",
    "                    if frameCount%int(fps*captionInterval)==0:\n",
    "                        timeStamp = frameCount/fps\n",
    "                        currentBatch.append((Image.fromarray(frame), timeStamp))\n",
    "                        if len(currentBatch)>=4:\n",
    "                            self.processCaptionBatch(currentBatch, captionInterval, cf)\n",
    "                            currentBatch = []\n",
    "                            \n",
    "                    # Object detection and tracking\n",
    "                    with torch.no_grad():\n",
    "                        detections = self.detectObjects(frame)\n",
    "                        filteredDetections = self.filterDetections(detections, frame, queryEmbedding, similarity)\n",
    "                        trackedDetections = self.tracker.update_with_detections(filteredDetections)\n",
    "                    # Annotate video frames\n",
    "                    annotatedFrame = self.annotateFrame(frame, trackedDetections)\n",
    "                    videoWriter.write(cv2.cvtColor(annotatedFrame, cv2.COLOR_RGB2BGR))\n",
    "                    frameCount+=1\n",
    "                    bar.update(1)\n",
    "            # Process remaining frames\n",
    "            if currentBatch:\n",
    "                self.processCaptionBatch(currentBatch, captionInterval, cf)\n",
    "        capture.release()\n",
    "        videoWriter.release()\n",
    "        for _, entry in enumerate(self.captionCache):\n",
    "            outputLine = f\"[{entry[\"start\"]}-->{entry[\"end\"]}] {entry[\"text\"]}\"\n",
    "            print(outputLine)\n",
    "        return outputPath\n",
    "        \n",
    "    def formatTimeStamp(self, seconds):\n",
    "        \"\"\"Convert seconds to HH:MM:SS.mmm\"\"\"\n",
    "        td = timedelta(seconds=seconds)\n",
    "        totalSeconds = td.total_seconds()\n",
    "        hours = int(totalSeconds//3600)\n",
    "        minutes = int((totalSeconds%3600)//60)\n",
    "        seconds = totalSeconds%60\n",
    "        return f\"{hours:02d}:{minutes:02d}:{seconds:06.3f}\"\n",
    "        \n",
    "    def processCaptionBatch(self, batch, interval, captionFile):\n",
    "        \"\"\"Process batch and captions\"\"\"\n",
    "        frames, timestamps = zip(*batch)\n",
    "        inputs = self.captionProcessor(images=frames, return_tensors=\"pt\").to(self.device, torch.float16)\n",
    "        with torch.no_grad():\n",
    "            generatedIDs = self.captionModel.generate(**inputs, max_new_tokens=50)\n",
    "        captions = self.captionProcessor.batch_decode(generatedIDs, skip_special_tokens=True)\n",
    "        for timestamp, caption in zip(timestamps, captions):\n",
    "            startTime = timestamp\n",
    "            endTime = timestamp+interval\n",
    "            startStr = self.formatTimeStamp(startTime)\n",
    "            endStr = self.formatTimeStamp(endTime)\n",
    "            captionEntry = {\"start\": startStr, \"end\": endStr, \"text\": caption.strip()}\n",
    "            self.captionCache.append(captionEntry)\n",
    "            outputLine = f\"[{startStr}-->{endStr}] {caption.strip()}\"\n",
    "            captionFile.write(outputLine+\"\\n\")\n",
    "        self.captionCache.sort(key=lambda x: x[\"start\"])\n",
    "        \n",
    "    def encodeQuery(self, query):\n",
    "        \"\"\"Encode query with CLIP\"\"\"\n",
    "        with torch.no_grad():\n",
    "            queryInput = clip.tokenize([query]).to(self.device)\n",
    "            return self.clipModel.encode_text(queryInput)\n",
    "            \n",
    "    def detectObjects(self, frame):\n",
    "        \"\"\"Object detection on a frame\"\"\"\n",
    "        inputs = self.detectionModelImageProcessor(images=Image.fromarray(frame), return_tensors=\"pt\").to(self.device)\n",
    "        outputs = self.detectionModel(**inputs)\n",
    "        results = self.detectionModelImageProcessor.post_process_object_detection(outputs, threshold=0.4, target_sizes=[(frame.shape[0], frame.shape[1])])[0]\n",
    "        return sv.Detections.from_transformers(results)\n",
    "        \n",
    "    def filterDetections(self, detections, frame, queryEmbedding, threshold):\n",
    "        \"\"\"Batch processing detection crops with CLIP\"\"\"\n",
    "        if len(detections)==0:\n",
    "            return detections\n",
    "        crops = []\n",
    "        validIndices = []\n",
    "        for idx, (x1, y1, x2, y2) in enumerate(detections.xyxy):\n",
    "            crop = frame[int(y1):int(y2), int(x1):int(x2)]\n",
    "            if crop.size>0:\n",
    "                crops.append(Image.fromarray(crop))\n",
    "                validIndices.append(idx)\n",
    "        if not crops:\n",
    "            return sv.Detections.empty()\n",
    "        clipInputs = torch.stack([self.clip_preprocess(crop) for crop in crops]).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            imageEmbedding = self.clipModel.encode_image(clipInputs)\n",
    "        similarities = F.cosine_similarity(queryEmbedding, imageEmbedding, dim=-1)\n",
    "        mask = (similarities>=threshold).cpu().numpy()\n",
    "        \n",
    "        #for idx, sim in zip(validIndices, similarities):\n",
    "            #print(f\"Box {idx} similarity: {sim.item()}\")\n",
    "        \n",
    "        filteredIndices = np.array(validIndices)[mask]\n",
    "        return detections[filteredIndices]\n",
    "        \n",
    "    def annotateFrame(self, frame, detections):\n",
    "        \"\"\"Draw annotations on frame\"\"\"\n",
    "        labels = [f\"#{tid} {self.detectionModel.config.id2label[cid]} {conf:.2f}\" for tid, cid, conf in zip(detections.tracker_id, detections.class_id, detections.confidence)]\n",
    "        annotated = self.boxAnnotator.annotate(scene=frame.copy(), detections=detections)\n",
    "        annotated = self.labelAnnotator.annotate(scene=annotated, detections=detections, labels=labels)\n",
    "        return annotated\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd058f6-043f-424f-8c4f-b6da94c5dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a81dd-587a-4d4c-aa00-23a90b20ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline.processVideo(videoPath=\"/Users/ivanng/Downloads/2103099-hd_1920_1080_30fps.mp4\", query=\"car\", outputPath=\"/Users/ivanng/annotated.mp4\", captionInterval=5, similarity=0.25, captionsFile=\"/Users/ivanng/annotatedCaptions.txt\")\n",
    "print(f\"\\nProcessed video save to {result}\")\n",
    "print(f\"captions saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f765cd-2f84-45c7-a1bf-e40fd8f6931e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
