{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6cd81-1bed-4294-b173-ef10374947c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "import supervision as sv\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "from collections import Counter\n",
    "from transformers import PaliGemmaProcessor,PaliGemmaForConditionalGeneration\n",
    "\n",
    "inference_mode = torch.inference_mode()\n",
    "inference_mode.__enter__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade3f4df-ad60-4bfb-9518-117ed6ba1b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    def __init__(self, device=torch.device(\"mps\")):\n",
    "        self.device = device\n",
    "        self.prompt = None\n",
    "        self.frameCount = 0\n",
    "        self.captionHistory = {}\n",
    "        self.initialiseModels()\n",
    "        \n",
    "    def initialiseModels(self):\n",
    "        self.detectionModel = Owlv2ForObjectDetection.from_pretrained(\"google/owlv2-base-patch16-ensemble\").to(self.device)\n",
    "        self.detectionModelProcessor = Owlv2Processor.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n",
    "        self.captionModel = PaliGemmaForConditionalGeneration.from_pretrained(\"google/paligemma2-3b-pt-224\", torch_dtype=torch.bfloat16).to(self.device).eval()\n",
    "        self.captionProcessor = PaliGemmaProcessor.from_pretrained(\"google/paligemma2-3b-pt-224\", use_fast=True)\n",
    "        self.boxAnnotator = sv.BoxAnnotator()\n",
    "        self.labelAnnotator = sv.LabelAnnotator()\n",
    "        self.captionCache = []\n",
    "        \n",
    "    def processVideo(self, videoPath, query, outputPath, captionInterval, captionsFile=\"captions.txt\"):\n",
    "        \"\"\"Main function for single pass execution\"\"\"\n",
    "        self.frameCount = 0\n",
    "        self.prompt = query\n",
    "        capture = cv2.VideoCapture(videoPath)\n",
    "        fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        totalFrames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        self.tracker = sv.ByteTrack(track_activation_threshold=0.38, lost_track_buffer=int(fps*2), minimum_matching_threshold=0.7, frame_rate=fps)\n",
    "        videoWriter = cv2.VideoWriter(outputPath, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "        with open(captionsFile, 'w') as cf:\n",
    "            with tqdm(total=totalFrames, desc=\"Processing Video\") as bar:\n",
    "                while capture.isOpened():\n",
    "                    ret, frame = capture.read()\n",
    "                    if not ret: break\n",
    "                    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                    timeStamp = self.frameCount/fps\n",
    "                    # Object detection and tracking\n",
    "                    with torch.no_grad():\n",
    "                        detections = self.detectObjects(frame)\n",
    "                        trackedDetections = self.tracker.update_with_detections(detections) \n",
    "                        \n",
    "                    # Caption generation\n",
    "                    if self.frameCount%int(fps*captionInterval)==0:\n",
    "                        self.generateRegionAwareCaptions(frame, trackedDetections, timeStamp)\n",
    "                        \n",
    "                    # Annotate video frames\n",
    "                    annotatedFrame = self.annotateFrame(frame, trackedDetections)\n",
    "                    videoWriter.write(cv2.cvtColor(annotatedFrame, cv2.COLOR_RGB2BGR))\n",
    "                    self.frameCount+=1\n",
    "                    bar.update(1)\n",
    "                \n",
    "                # finalise captions   \n",
    "                self.finaliseCaptions(cf)\n",
    "                \n",
    "        capture.release()\n",
    "        videoWriter.release()\n",
    "        for _, entry in enumerate(self.captionCache):\n",
    "            outputLine = f\"[{entry['start']}-->{entry['end']}] (Object #{entry['trackerID']}): {entry['text']}\"\n",
    "            print(outputLine)\n",
    "        return outputPath\n",
    "    \n",
    "    def formatTimeStamp(self, seconds):\n",
    "        \"\"\"Convert seconds to HH:MM:SS.mmm\"\"\"\n",
    "        td = timedelta(seconds=seconds)\n",
    "        totalSeconds = td.total_seconds()\n",
    "        hours = int(totalSeconds//3600)\n",
    "        minutes = int((totalSeconds%3600)//60)\n",
    "        seconds = totalSeconds%60\n",
    "        return f\"{hours:02d}:{minutes:02d}:{seconds:06.3f}\"\n",
    "        \n",
    "    def finaliseCaptions(self, captionFile):\n",
    "        \"\"\"Process and write captions in file\"\"\"\n",
    "        sortedCaptions = sorted(self.captionHistory.items(), key=lambda x: x[1]['start'])\n",
    "        for trackid, entry in sortedCaptions:\n",
    "            duration = max(0, entry['end'] - entry['start'])\n",
    "            start = self.formatTimeStamp(entry['start'])\n",
    "            end = self.formatTimeStamp(entry['start'] + duration)\n",
    "            \n",
    "            captionsCounter = Counter([cap['text'] for cap in entry['captions']])\n",
    "            mostFrequentCaption = captionsCounter.most_common(1)\n",
    "            captionText = mostFrequentCaption[0][0]\n",
    "            captionLine = f\"[{start} --> {end}] (Object #{trackid}): {captionText}\"\n",
    "            self.captionCache.append({\"start\": start, \"end\": end, \"trackerID\": trackid, \"text\": captionText})\n",
    "            captionFile.write(captionLine + \"\\n\")\n",
    "\n",
    "    def generateRegionAwareCaptions(self, frame, detections, timestamp):\n",
    "        pil_frame = Image.fromarray(frame)\n",
    "        width, height = pil_frame.size\n",
    "        prompts = []\n",
    "        tracker_ids = []\n",
    "        for idx, (x1, y1, x2, y2) in enumerate(detections.xyxy):\n",
    "            tracker_id = detections.tracker_id[idx]\n",
    "            if tracker_id not in self.captionHistory:\n",
    "                self.captionHistory[tracker_id] = {'start': timestamp, 'end': timestamp, 'captions': []}\n",
    "            else:\n",
    "                self.captionHistory[tracker_id]['end'] = timestamp\n",
    "            x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "            if x2 <= x1 or y2 <= y1:\n",
    "                continue\n",
    "            loc_x1 = int((x1 / width)*1024)\n",
    "            loc_y1 = int((y1 / height)*1024)\n",
    "            loc_x2 = int((x2 / width)*1024)\n",
    "            loc_y2 = int((y2 / height)*1024)\n",
    "            loc_x1, loc_y1 = max(0, min(loc_x1, 1023)), max(0, min(loc_y1, 1023))\n",
    "            loc_x2, loc_y2 = max(0, min(loc_x2, 1023)), max(0, min(loc_y2, 1023))\n",
    "            prompt = f\"<image>caption en <loc{loc_y1:04d}><loc{loc_x1:04d}><loc{loc_y2:04d}><loc{loc_x2:04d}>\\n\"\n",
    "            prompts.append(prompt)\n",
    "            tracker_ids.append(tracker_id)\n",
    "        with torch.no_grad():\n",
    "            images = [pil_frame]*len(prompts)\n",
    "            inputs = self.captionProcessor(images=images, text=prompts, padding=True, return_tensors=\"pt\").to(self.device)\n",
    "            outputs = self.captionModel.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "            input_len = inputs[\"input_ids\"].shape[-1]\n",
    "            decoded = [self.captionProcessor.decode(output[input_len:], skip_special_tokens=True) for output in outputs]\n",
    "            for tracker_id, caption in zip(tracker_ids, decoded):\n",
    "                self.captionHistory[tracker_id]['captions'].append({'time': timestamp, 'text': caption})\n",
    "    \n",
    "    def detectObjects(self, frame):\n",
    "        \"\"\"Object detection on a frame\"\"\"\n",
    "        pil_frame = Image.fromarray(frame)\n",
    "        inputs = self.detectionModelProcessor(text=[self.prompt], images=pil_frame, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.detectionModel(**inputs)\n",
    "        target_sizes = torch.tensor([pil_frame.size[::-1]]).to(self.device)\n",
    "        results = self.detectionModelProcessor.post_process_object_detection(outputs=outputs, threshold=0.38, target_sizes=target_sizes)[0]\n",
    "        return sv.Detections.from_transformers(results)\n",
    "        \n",
    "    def annotateFrame(self, frame, detections):\n",
    "        \"\"\"Draw annotations on frame\"\"\"\n",
    "        labels = [f\"#{tid} {self.prompt[cid]} {conf:.2f}\" for tid, cid, conf in zip(detections.tracker_id, detections.class_id, detections.confidence)]\n",
    "        annotated = self.boxAnnotator.annotate(scene=frame.copy(), detections=detections)\n",
    "        annotated = self.labelAnnotator.annotate(scene=annotated, detections=detections, labels=labels)\n",
    "        return annotated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd058f6-043f-424f-8c4f-b6da94c5dd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "pipeline = Pipeline()\n",
    "pipeline.captionCache.clear()\n",
    "pipeline.captionHistory.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0a81dd-587a-4d4c-aa00-23a90b20ab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Require video path and object of interest\n",
    "result = pipeline.processVideo(videoPath=\"\", query=[\"\"], outputPath=\"annotated.mp4\", captionInterval=5, captionsFile=\"annotatedCaptions.txt\")\n",
    "print(f\"\\nProcessed video save to {result}\")\n",
    "print(f\"captions saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbfd827-e3b2-402c-a82e-b50c1f7d3a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.captionHistory.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5401b-8ce3-4cfd-b8da-42ec789e5589",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
